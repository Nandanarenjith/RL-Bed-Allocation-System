import numpy as np
import random

# Define the state space (simplified for demonstration)
# Using 'Severity_Score' and 'Length_of_Stay' as state features.
# In a real application, a more comprehensive representation is needed.
state_space_size = (10, 10)  # Discretize Severity and Length of Stay into 10 levels each.
action_space_size = 2  # 0: No bed allocation, 1: Allocate bed

# Initialize the Q-table
q_table = np.zeros((state_space_size[0], state_space_size[1], action_space_size))

# Hyperparameters
learning_rate = 0.1
discount_rate = 0.95
exploration_rate = 1.0
max_exploration_rate = 1.0
min_exploration_rate = 0.01
exploration_decay_rate = 0.01


def discretize_state(state):
    severity = int(state[3] * 9)  # Severity_Score
    los = int(state[2] * 9)     # Length_of_Stay
    return severity, los

# Training loop (simplified for demonstration)
episodes = 1000 # Limit the episodes due to resource limits
for episode in range(episodes):
    state_index = discretize_state(X_train.iloc[episode % len(X_train)])  # Sample states from training data
    done = False

    while not done:
        # Exploration-exploitation trade-off
        exploration_rate_threshold = random.uniform(0, 1)
        if exploration_rate_threshold > exploration_rate:
            action = np.argmax(q_table[state_index])
        else:
            action = random.choice(np.arange(action_space_size))

        # Simplified reward (needs refinement based on actual ICU metrics)
        if action == 1 and state_index[0] > 5:  # Reward allocating bed for high severity
            reward = 1
        else:
            reward = 0

        # Transition to the next state
        next_state_index = discretize_state(X_train.iloc[(episode + 1) % len(X_train)])

        # Q-table update rule
        q_table[state_index][action] = q_table[state_index][action] * (1 - learning_rate) + \
                                         learning_rate * (reward + discount_rate * np.max(q_table[next_state_index]))

        state_index = next_state_index

        # Decay exploration rate
        exploration_rate = min_exploration_rate + \
                           (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)

        done = True  # Simplified termination condition


print("Q-table:", q_table)
#reasoning
import numpy as np
import random
import pickle

# Define the state space (simplified for demonstration)
state_space_size = (10, 10)  # Discretize Severity and Length of Stay
action_space_size = 2  # 0: No bed allocation, 1: Allocate bed

# Initialize the Q-table
q_table = np.zeros((state_space_size[0], state_space_size[1], action_space_size))

# Hyperparameters
learning_rate = 0.1
discount_rate = 0.95
exploration_rate = 1.0
max_exploration_rate = 1.0
min_exploration_rate = 0.01
exploration_decay_rate = 0.01

def discretize_state(state):
    severity = int(state.iloc[3] * 9)
    los = int(state.iloc[2] * 9)
    return severity, los

# Training loop
episodes = 1000
for episode in range(episodes):
    state_index = discretize_state(X_train.iloc[episode % len(X_train)])
    done = False
    while not done:
        exploration_rate_threshold = random.uniform(0, 1)
        if exploration_rate_threshold > exploration_rate:
            action = np.argmax(q_table[state_index])
        else:
            action = random.choice(np.arange(action_space_size))

        # Reward function (still simplified)
        next_state_index = discretize_state(X_train.iloc[(episode + 1) % len(X_train)])
        reward = 0
        if action == 1 and next_state_index[0] < state_index[0]: # Reward if severity decreases
            reward = 1
        elif action == 1 and next_state_index[0] >= state_index[0]: # Penalty if severity increases
            reward = -0.5

        q_table[state_index][action] = q_table[state_index][action] * (1 - learning_rate) + \
            learning_rate * (reward + discount_rate * np.max(q_table[next_state_index]))

        state_index = next_state_index
        exploration_rate = min_exploration_rate + \
                           (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)
        done = True

# Save the trained model
try:
    with open('icu_bed_allocation_model.pkl', 'wb') as file:
        pickle.dump(q_table, file)
    print("Model saved successfully.")
except Exception as e:
    print(f"Error saving the model: {e}")

print("Q-table:", q_table)
