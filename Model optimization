import numpy as np
import random
import pickle
from sklearn.model_selection import ParameterGrid

try:
    with open('icu_bed_allocation_model.pkl', 'rb') as file:
        q_table = pickle.load(file)
    print("Model loaded successfully.")
except FileNotFoundError:
    print("Error: 'icu_bed_allocation_model.pkl' not found.")

# Define hyperparameter ranges
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'discount_rate': [0.9, 0.95, 0.99],
    'exploration_decay_rate': [0.001, 0.01, 0.1]
}

# Helper functions (same as before)
state_space_size = (10, 10)
action_space_size = 2

def discretize_state(state):
    severity = int(state.iloc[3] * 9)
    los = int(state.iloc[2] * 9)
    return severity, los

# Perform grid search
best_params = None
best_avg_reward = -np.inf

for params in ParameterGrid(param_grid):
    print(f"Evaluating params: {params}")
    q_table_copy = np.copy(q_table)  # Reset Q-table for each parameter set
    learning_rate = params['learning_rate']
    discount_rate = params['discount_rate']
    exploration_decay_rate = params['exploration_decay_rate']
    exploration_rate = 1.0
    max_exploration_rate = 1.0
    min_exploration_rate = 0.01

    avg_rewards = []
    episodes = 100 # Reduced for faster evaluation
    for episode in range(episodes):
        state_index = discretize_state(X_val.iloc[episode % len(X_val)])  # Use validation set
        total_reward = 0
        done = False

        while not done:
            # Similar action/reward calculation
            exploration_rate_threshold = random.uniform(0, 1)
            if exploration_rate_threshold > exploration_rate:
              action = np.argmax(q_table_copy[state_index])
            else:
              action = random.choice(np.arange(action_space_size))

            next_state_index = discretize_state(X_val.iloc[(episode + 1) % len(X_val)])
            reward = 0
            if action == 1 and next_state_index[0] < state_index[0]:
                reward = 1
            elif action == 1 and next_state_index[0] >= state_index[0]:
                reward = -0.5

            q_table_copy[state_index][action] = q_table_copy[state_index][action] * (1 - learning_rate) + \
                                               learning_rate * (reward + discount_rate * np.max(q_table_copy[next_state_index]))
            total_reward += reward
            state_index = next_state_index
            exploration_rate = min_exploration_rate + \
                               (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)
            done = True
        avg_rewards.append(total_reward)
    avg_reward = np.mean(avg_rewards)
    print(f"Average reward: {avg_reward}")
    if avg_reward > best_avg_reward:
        best_avg_reward = avg_reward
        best_params = params
import pickle

# Retrain with best hyperparameters (best_params) and combined data
print(f"Best hyperparameters: {best_params}")

# Assuming best_params is available from the previous step
# Example (replace with the actual best_params):
# best_params = {'discount_rate': 0.99, 'exploration_decay_rate': 0.001, 'learning_rate': 0.01}

if best_params:
    q_table_optimized = np.zeros((state_space_size[0], state_space_size[1], action_space_size))
    learning_rate = best_params['learning_rate']
    discount_rate = best_params['discount_rate']
    exploration_decay_rate = best_params['exploration_decay_rate']
    exploration_rate = 1.0
    max_exploration_rate = 1.0
    min_exploration_rate = 0.01

    combined_data = pd.concat([X_train, X_val], ignore_index=True)

    episodes = 1000  # Retrain for the same number of episodes
    for episode in range(episodes):
        state_index = discretize_state(combined_data.iloc[episode % len(combined_data)])
        done = False
        while not done:
            exploration_rate_threshold = random.uniform(0, 1)
            if exploration_rate_threshold > exploration_rate:
                action = np.argmax(q_table_optimized[state_index])
            else:
                action = random.choice(np.arange(action_space_size))

            next_state_index = discretize_state(combined_data.iloc[(episode + 1) % len(combined_data)])
            reward = 0
            if action == 1 and next_state_index[0] < state_index[0]:
                reward = 1
            elif action == 1 and next_state_index[0] >= state_index[0]:
                reward = -0.5

            q_table_optimized[state_index][action] = q_table_optimized[state_index][action] * (1 - learning_rate) + \
                learning_rate * (reward + discount_rate * np.max(q_table_optimized[next_state_index]))
            state_index = next_state_index
            exploration_rate = min_exploration_rate + \
                (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)
            done = True

    try:
        with open('optimized_icu_bed_allocation_model.pkl', 'wb') as file:
            pickle.dump(q_table_optimized, file)
        print("Optimized model saved successfully.")
    except Exception as e:
        print(f"Error saving the optimized model: {e}")
else:
    print("No best hyperparameters found during the grid search.")
